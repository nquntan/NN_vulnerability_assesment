import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

from art.attacks import FastGradientMethod
from art.classifiers import PyTorchClassifier
from model.vgg import *
from art.utils import load_dataset

import random
from art.utils import projection
from art.utils import random_sphere

import os
os.environ["CUDA_VISIBLE_DEVICES"]="1,2"

# Step 0: Define the neural network model, return logits instead of activation in forward method

# Step 1: Load the MNIST dataset

(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str('cifar10'))

# Step 1a: Swap axes to PyTorch's NCHW format

x_train = np.swapaxes(x_train, 1, 3).astype(np.float32)
x_test = np.swapaxes(x_test, 1, 3).astype(np.float32)

# Step 2: Create the model

vgg_ver = "VGG16"
model = VGG(vgg_ver)

# Step 2a: Define the loss function and the optimizer

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-2)

# Step 3: Create the ART classifier

classifier = PyTorchClassifier(model=model, clip_values=(min_, max_ ), loss=criterion,
                               optimizer=optimizer, input_shape=(3, 32, 32), nb_classes=10)

# Step 4: Train the ART classifier

classifier.fit(x_train, y_train, batch_size=128, nb_epochs=30)
classifier.save(f"pytorch_{vgg_ver}", "./logs")

# Step 5: Evaluate the ART classifier on benign test examples

predictions = classifier.predict(x_test)
accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)
print('Accuracy on benign test examples: {}%'.format(accuracy * 100))

